Rant: Some of the existing soluions in big data processing are heavily optimized and the controolled environment in which these execute. How can these be transposed to the containerized worlds, with maintaining the benefits of containers and separation of oncenrns. Measuring the benefits to pick the most significant improvements.

Containers promise a lot of benefits which would only be dreamed of wihtout them, but it's important to take into account that big data processing needs to be efficient and performant to keep the cost down and keep up with the requirement in terms of velocity, volume etc.

Add the figure with the separation of concerns somewhere in the background section.
    Explain what each concern is responsbile for:
        1. Data - companies exposing the data they have - data marketplace
        2. Compute - the steps that process the data
        3. Workflow creator - Subject matter expert that knows how to combine the data and the steps 
        4. Infrastructure - people responsible with maintaining the infrastructure on which the processing needs to happen
        5. Platform developer. - people responsbile with developing frameworks that can execute workflows using the specified data and computation steps on top of the infrastructure.

In the problem framing section, add the information that the current work studies a different approach for data locality [and other technological approaches] with the end goal of reducing the execution time of workflows and the required bandwidth utilization.

A better approach for data locality that does not intere a lot with the other concerns.
Runtime aspects (and what the platform developers should offer):

    Figure about data locality, exlplaining the actual problem of data locality.
        In general, with Hadoop and other offerings that handle this outside of the box, but with the limitations inherent to these platforms - they are not extensible and require a lot of platform specific knowledge to operate.

    Communication overhead = add a short explanation why serialization / transmission time matters. 
        Frame it as something we need to keep in mind (that other platforms may or may not be doing already)
        And something we measure the impact of. The measurement being the key word here. 

    Container lifecycle management = again this may not be new, but we propose it as an improvement.
        Again, measuring the impact of it is the contribution the thesis brings to the research field.


Move stuff from the dumping ground to their appropriate chapters.

Split the background file into multiple manageable files.

Start off by identifying the general pattern for the separation of concerns:
    Steps are isoalted in their own containers,
    Platform developers provide a way for users to inject their desired behavior via these container
    For workflow designers, the platform may or may not expose a certain DSL.

Finish by highlighting where the approach works and where the approach doesn't work - via experiments
1st of all - synthetic experiments.
Shared containers are actually part of the larger topic of deployment:
    1. Deploy 1 container per unit of data that needs to be processed
    2. Deploy containers statically.
    3. Dynamically provision containers in the locations they are needed in terms of data locality.
Figure with the taxonomy of things in the design - separation of concerns
As I write stuff in other chapters, add markers on them like : Challenge #X, Trend #X. - so as to draw attention to them we introduce them in the grand context of things.


What is the best part of my approach for big data locality. - what makes it different.

How to introduce the concept of workflows vs big data in general.

Big data workflows, data locality

Where to introduce the separation of concerns figure? - in the microservices world. Software containers serve as the enabling catalyst that makes this vision a reality.

Software containers give the framework control over the deployment by having a standardized way of specifying what it needs to have deployed.

Not complicating the design/ coding aspect of the workflows.


For tomorrow:

    Intro part.
    A lot from the Intro/Motivation should actually either live in the context or in the background sections.

    What are big data workflows in general? 
        Pipelines moving, transforming data and potentially performing some quick analysis on them. != offline analysis. Put data into data lakes maybe?
        Many tools are usually involved in this process. Oozie is the go-to solution for hadoop.
        Integration and orchestration of multiple tools is an significant challenge in this world. Thus come software containers.
            Hadoop specific.

        Give an example. The initial survey paper I used for the essay contains an example
        The approach I'm proposing can be extend past the simplistic workflow model and be used as a generic abstraction for contianerizing and shipping  code that can communicate with the platform behind it, not necessarily via SDKs but possible via microservices communication.

        The offline analytics can become part of

    Figures - list and start drawing. - Add placeholders wherever I want to add a figure.  


    List becoming sections/subsection/subsub instead?

    Go into the design and implementation sections and fill them in.   

Why do I keep rambling about the requiremnets/ keeping in sync with the existing state of the art? Because for performance reasons it is easy to optimize and lose track of benefits targeted in the first place.

Variety is the source of value - you can break down silos. Example? Traffic monitoring + weather data.

RQ1 : What is the benefit of incorporating data locality in technology-agnostic communication

Pivot everything on value. How do we bring value? 

Data locality  -> makes it possible to execute faster and cheaper - more value.


Highlight definitions as we go along:
    -> Computing continuum

To the best of our knowledge, we are the first to add data locality concepts in a microservice oriented architecture, container based big data workflow system.


Related works ++ Flyte (from Lyft). Dagster doesn't support wrapping steps in containers.

Feedback:
    Related works to include stuff about data locality

    Software containers and how they are used in the big data world - different taxonomies. 