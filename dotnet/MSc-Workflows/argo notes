For argo stuff:

Need to adapt the compute step.
    To output the zone where it pushed the data.
    To output the locality preference
        -> With TopologySpreadConstraints 


Use a bash script to generate the workflow with the list 
    UseWithItems to send multiple requests in parallel

WithItems + PodSpecPatch for Argo workflows.
PodSpecPatch to include TopologySpreadConstraints and differennt localities maybe.


This PodSpecPatch with references to the previous steps could be used to simulate the exact same data locality as my solution.

Each step has to have knowledge of the cluster topology (who is close to what)
My solution with leverages data locality while maintaining separation of concers 
    (data locality concerns don't bleed into the workflow definiton)
PodSpecPatches need to know the state of the cluster.
First step doesn't know where the data originates from - unless injected by trigger.



Can the PodSpecPatch be of a complex type or just a normal value

Azure Blob storage uses compression by default.
    Gzip - on the wire.

Storage account location is West Europe.


Edge1 host1 : 20.56.88.169