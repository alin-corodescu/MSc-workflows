Intro & Background:
    Directions in this general field:
        -> Data marketplace : Different, heterogenous data sources.
        -> Interopearbility between different technologies
        -> Edge ready

    Design and implement a platform. Design and implementation influenced greatly by the research and industry directions. Meant to capture the performance benefits of design and implementation choices without sacrificing on the benefits and the without turning completely against the current direction.

    Understanding the underlying motivations of design/ implementation choices of the existing solutions. container technologies is important - and we will keep in these in mind 

    How to optimize a container based orchestration solution for big data workloads.
    Not a complete solution.

Story:
    Define the control flow and data flow as they are relevant for the framing of the problem.

    For data flow optimzation:  
        Data locality, Hard linking.
        For workflows where data flow is a large chunk of the overall cost.

    Starting from the assumption that containerizing workflows can have a detrimental effect to performance due to the need of data serialization and communication that is not in-memory.
        -> For workflows where the control flow communication is intensive, optimize the communication betwen the components.
         

    [ Lifecycle management, especially in the context of edge computing - continous streams of data.]

    [
        Data flow solution: 
        
        Architecture is compatible with: Using different data storage adapters allowing us to have data adapters that can be plugged in by the user as long as they adhere to an interface.
            The choice is up to the implementer to decide if they can provide data locality information.
        
        Include it in the artifact summary at the end.
    ]

Design
    Perhaps introduce a taxonomy for the different layers involved in the creation and execution of the workflow.
    Why containers
    Why Kubernetes - abstracts the hardware away for us, and allows us to orchestrate the workflows over heterogenous hardware.
        Unifies the experience of controlling the different devices through a common API.
    List challenges here (taxonomy - like the challenges with Control plane, data plane)
    Microservices architecture
        Separation of concerns + the benefits. 
    Components:
        => The sidecar + orchestrator == contribute to the orchestration of the workflow.
            Similar to the Kubernetes architecture of Kubernetes, perhaps it would be possible to transpose this solution to integrate natively with Kubernetes.
            Like Argo workflows is implemented.
        => Steps + Data solution interactor. == provided by the user of the framework
        => the interaction between them, at a conceptual level.


    Data locality design part.
        Mention that data locality is achieved in he cooperation between the control component and the data component
        
        The design accounts for automatic handling of data handling without the intervention of upper layers (like the person writing the workflow)
            -> but the it allows for explicit specification of data locality requirements in case of GDPR or other compliance with regulations.
        
        Why is data locality up to the data layer and not the control layer.
            -> Data is possible to be reusable  across workflows and for other purposes and having the data locality exposed by the data layer allows other workflows/ applications/ use-cases take advantage of the available  
    
    Shared containers and communication overhead reduction.. maybe I can leave these for the implementation details section.


Implementation

    Implementation that exemplifies the proposed architecture and allows to evaluate the design choices.

    ASP.NET services
        GRPC services


    IDL = Protobuf - a good option for defining the interfaces for communication between the components
        Standardized communication for inter-service communication
        Comprasion with REST from the perspective of inter usability.
        Comparison with some primitive file-based notification or something. -- Describe Yared's thesis
        
        The protobuf implementation can capture parameters at runtime, parameters describing how data should be processed are not captured as part of the container creation step.

    Deployment model : 1 control plane, 1 daemon set for each data solution

    Kubernetes abstractions used (Services, Daemon sets)
        Sidecar pattern for the communication with the outside world.
        Injecting communication methods via environment variables or other Kubernetes featurs.
        Using Kubernetes to get the current state of the cluster (possible nodes, etc.)
            While not necessarily the focus of the thesis, it's worth calling out as simplifies the things significantly.
        
    Local File system adapter - supporting data locality
        Data + isolation:
            Data flow optimization (performance) : Hard linking vs copying the files around on the same host
                Explain why this works in the context of containers
                Maintains the advantage of separation - otherwise we could just mount the same volume everywhere and pass that around 
                Most workflows will download the data locally for the step to process.

    Control plane limitation: 
        -> Doesn't support offline (autonomous operation of )
        -> Not distributed (can become a bottleneck)
        -> Should ideally be present in multiple geos to be able to orchestrate workflows

    
        Data locality - orchestration level - smarter decisions about the data.
            Requires intimate knowledge about the executed workflow. 
        Shared containers - container/ virtualization specific


    Inter container communication:
        REST over HTTP, gRPC over HTTP2, [MQTT]
        Establishing HTTP connection can play a significant role:
            in 
            [GRPC streaming vs non-streaming solution] -- according to some SO threads this can actually degrade performance (apperantly backed by google themselves)
            GRPC channel pool




How to frame the other improvements? Basically I'm saying that the implementation is not the focus of the thesis, yet

Architectural and technological approaches to optimize the 

    


We are not trying to improve a specific implementation, instead or we studying architectural and technological approaches to find improvement opportunities in the area of efficiently orchestrating big data worfklows across heterogenuous, geo-distributed resources and learnings that can be re-applied to  other implementations.


Conclusion:

What did we do in the end?
    We analyzed the state of the art to identify the direcitons in which the research/ industry headed. (by analyzing the underlying motivations for implementations/ features)
    And current trends. Interoperability, sep of concerns. better suited for execution on the computing continuum. 
        Interop and sep of concerns is facilitated by the use of software containers.
        Scalability
    Peformance investigations based on other papers (data locality, shared containers thingy, I can add ). Advantages studied in isolation.

    Based on the requirements imposed by big data and edge computing
        The thesis focus will be on performance of the workflows + isolation and separation of concerns introducing additional overhead.
            We identified data locality as a problem which has not been solved.
                We proposed a set of abstractions that have data locality as a covered aspect.
                We created efficient implementations for the proposed abstractions

            We also analyzed other strategies of improving the performance of the workflows.
                We attempt to identify the problems at a generic level Shared containers, (such as HTTP connection pooling), payload size, etc.
                And provide implementations that prove our ideas. Not specific to our implementation, underlying ideas that can be re-used irresepctive of the implementation. Generally applicable patterns and their impact.
        The proposed design is also mindful of  the current trends and directions.
    


Try to aim for the minimal implementation that can be experimented upon to produce some valuable output.
    -> Data locality implementation
        + Bandwidth and latency, throughtput savings


diagrams:
Diagrams for the experiments explaining what each number captures exactly



Explain everything in very great detail!!! Don't assume anything. Diagrams, explanations, etc.

Peformance and efficiency may not be the best wording

For each section, have a summarizing sentence - one-liner for each 

Who is the end-user of the "framework" who are the stakeholders who will benefit from the improvement presented in the thesis.  
    -- use the word approach instead of framework.
    Approach instead of framework and the end user who will benefit from this approach are workflow system developers directly by incorporating the approaches here into their solutions, as well as the workflows creator, indirectly, by enabling newer, previously costly scenarios (edge deployment)


How to communicate the key features of the approach - architecture, technological approaches?
    It's a flexible architecture that allows for extension. And it accepts data locality as first class citizen.
    So it's architecture (data locality) but also technology specific approaches (hard linking, GRPC as an efficient protocol, )

Example - why are other approaches bad and mine is better?
    Before and after
    Exemplify (and draw diagrams explaining why data locality is a problem - and how others solve/ don't solve the issue)
    What does it imply in terms of changes?
        - No large impact to the person designing the workfow - make the improvements work behind the scenes as well as possible. Impact as little as possible in terms of users other than the developer of the workflow system.

        - Changes in the deployment/ coding etc. (any process - tag your pods)


Teams folder: 
    Approahes for Big data workflows: - for related works state of the art

Be super specific in the goals and requirements. What do I intend to do. 
    

Script for my thesis: bullet points indicating the logical flow of my contributions.


How do the containers actually help with the data locality - compute shipping.

What cases are these useful for actually? Call this out explictly
    What are some examples of the workflows - give details.
    Runtime vs deployment
        Data dynamically moving. Deployment is static . Changes into data layout needs to be reflected in the topology. Most of the existing frameworks rely on the container orchestration capabilities and as such extend the deployment capabilites to provision the compute steps. However, this approach becomes fragile when these provisioning operations are frequent and need to operate on very little data.

    Data locality: bring computation close to the data. Containerization hepls with this by standardizing

How do containers help with separation of concerns - what do I exactly mean about the separation of concerns here. - include things from the DSL.


Add introduction about big data and stuff.
    More stuff from the edge.

Add figure about the separation of concerns